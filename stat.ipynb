{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3493cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "#import sys\n",
    "from scipy.sparse import csr_matrix\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from pyspark.sql.functions import expr, to_timestamp, col, unix_timestamp, first, sum, count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import LongType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb1a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datapath file CSV (dataset)\n",
    "pathCSV = 'data/trips2323_new.csv'\n",
    "\n",
    "# Path ShapeFile\n",
    "pathShapeF = 'data/shapefile/shapefile_2323.shp'\n",
    "\n",
    "# Path for results\n",
    "pathRes = 'results/'\n",
    "\n",
    "# Skip division per 0\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    #df_cache = df.spark.cache()\n",
    "    df = df.rename(columns={'from_zone':'from_zone_fid', 'from_timedate':'from_timedate_gmt', 'to_zone':'to_zone_fid', 'to_timedate':'to_timedate_gmt'})\n",
    "\n",
    "    df = df.dropna(subset=['to_zone_fid'])\n",
    "    fill = {'from_zone_fid': -1}\n",
    "    df = df.fillna(value=fill)\n",
    "\n",
    "\n",
    "    # Initial Filter\n",
    "    # (df.triptime_s < 3278)\n",
    "    df = df.loc[ (df.tripdistance_m > 0) & (df.stoptime_s > 0)]\n",
    "\n",
    "    #df = df.sort_values(by=['from_timedate_gmt', 'to_timedate_gmt'])\n",
    "    #df = df.reset_index(drop = True)\n",
    "\n",
    "    df_spark = df.to_spark()\n",
    "\n",
    "    df_spark = df_spark.withColumn(\"from_timedate_gmt\", expr(\"substring(from_timedate_gmt, 1, length(from_timedate_gmt)-3)\"))\n",
    "    df_spark = df_spark.withColumn(\"to_timedate_gmt\", expr(\"substring(to_timedate_gmt, 1, length(to_timedate_gmt)-3)\"))\n",
    "    #df_spark = df_spark.withColumn(\"from_timedate_gmt\", df_spark.from_timedate_gmt.substr(1, 6))\n",
    "\n",
    "    df = df_spark.to_koalas()\n",
    "\n",
    "    #df_cache.spark.unpersist()\n",
    "\n",
    "    return df\n",
    "# second to hour\n",
    "def square(s):\n",
    "    return s/3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495bc3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 18:08:20 WARN Utils: Your hostname, gpu2-Standard-PC-Q35-ICH9-2009 resolves to a loopback address: 127.0.1.1; using 10.0.0.205 instead (on interface enp6s18)\n",
      "22/06/14 18:08:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gpu2/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/14 18:08:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/14 18:08:22 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------OUTPUT------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 18:09:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:09:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:10:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "/home/gpu2/.local/lib/python3.8/site-packages/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[1]\")\\\n",
    "        .appName(\"Pyspark\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "spark.catalog.clearCache()\n",
    "    \n",
    "    \n",
    "# Upload CSV datas\n",
    "df = ks.read_csv(pathCSV, sep=';', dtype = {'from_timedate':str, 'to_timedate':str})\n",
    "#df = df.spark.cache()\n",
    "df = df.spark.repartition(30)\n",
    "df = preprocessing(df)\n",
    "#   print(df.head())\n",
    "#   exit()\n",
    "print('-----------OUTPUT------------')\n",
    "#df_cache = df.spark.cache()\n",
    "df = df.rename(columns={'from_zone':'from_zone_fid', 'from_timedate':'from_timedate_gmt', 'to_zone':'to_zone_fid', 'to_timedate':'to_timedate_gmt'})\n",
    "df = df.dropna(subset=['to_zone_fid'])\n",
    "fill = {'from_zone_fid': -1}\n",
    "df = df.fillna(value=fill)\n",
    "# Iitial Filter\n",
    "# (f.triptime_s < 3278)\n",
    "df = df.loc[ (df.tripdistance_m > 0) & (df.stoptime_s > 0)]# & (df.to_zone_fid != -1)]\n",
    "#print(df.stoptime_s.max())\n",
    "df.stoptime_s = df.stoptime_s.apply(square)\n",
    "#test = df.stoptime_s.to_numpy()\n",
    "\n",
    "#ax = \n",
    "#df.stoptime_s.plot.kde(bw_method=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9e28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 16:25:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 16:25:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 16:26:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 28:=========================================>              (22 + 1) / 30]\r"
     ]
    }
   ],
   "source": [
    "df.loc[ df.stoptime_s < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e958ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoptime_bound(df):\n",
    "    #Upper Bound e Lower Bound\n",
    "    print(df.stoptime_s.describe())\n",
    "    Q1 = df.stoptime_s.describe().to_list()[4]\n",
    "    Q2 = df.stoptime_s.describe().to_list()[5]\n",
    "    Q3 = df.stoptime_s.describe().to_list()[6]\n",
    "    IQR = Q3 - Q1\n",
    "    L = Q1 - (1.5 * IQR)\n",
    "    U = Q3 + (1.5 * IQR)\n",
    "    print('Lower: ', L)\n",
    "    print('Upper', U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffc9316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 18:10:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:11:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:11:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.336024e+07\n",
      "mean     5.320176e+00\n",
      "std      2.853858e+01\n",
      "min      2.777778e-04\n",
      "25%      1.402778e-01\n",
      "50%      7.661111e-01\n",
      "75%      4.816667e+00\n",
      "max      8.728416e+03\n",
      "Name: stoptime_s, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 18:11:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:12:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:12:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "22/06/14 18:13:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:14:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:14:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:14:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:15:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/14 18:15:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower:  -6.874305555555556\n",
      "Upper 11.83125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stoptime_bound(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad655fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13360243"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c273f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/07 10:47:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/07 10:48:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/07 10:48:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11751142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[ df.stoptime_s < 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4529d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1589340"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13340482 - 11751142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9426a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11913662489856064"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1589340/13340482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fdb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stoptime_s.plot.kde(bw_method=0.001)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

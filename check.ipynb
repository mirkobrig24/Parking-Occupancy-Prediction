{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3493cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "#import sys\n",
    "from scipy.sparse import csr_matrix\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from pyspark.sql.functions import expr, to_timestamp, col, unix_timestamp, first, sum, count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import LongType, IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb1a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datapath file CSV (dataset)\n",
    "pathCSV = 'data/trips2323_new.csv'\n",
    "\n",
    "# Path ShapeFile\n",
    "pathShapeF = 'data/shapefile/shapefile_2323.shp'\n",
    "\n",
    "# Path for results\n",
    "pathRes = 'results/'\n",
    "\n",
    "# Skip division per 0\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    #df_cache = df.spark.cache()\n",
    "    df = df.rename(columns={'from_zone':'from_zone_fid', 'from_timedate':'from_timedate_gmt', 'to_zone':'to_zone_fid', 'to_timedate':'to_timedate_gmt'})\n",
    "\n",
    "    df = df.dropna(subset=['to_zone_fid'])\n",
    "    fill = {'from_zone_fid': -1}\n",
    "    df = df.fillna(value=fill)\n",
    "\n",
    "\n",
    "    # Initial Filter\n",
    "    # (df.triptime_s < 3278)\n",
    "    df = df.loc[ (df.tripdistance_m > 0) & (df.stoptime_s > 0)]# & (df.stoptime_s < 604800)]\n",
    "\n",
    "    #df = df.sort_values(by=['from_timedate_gmt', 'to_timedate_gmt'])\n",
    "    #df = df.reset_index(drop = True)\n",
    "\n",
    "    df_spark = df.to_spark()\n",
    "\n",
    "    df_spark = df_spark.withColumn(\"from_timedate_gmt\", expr(\"substring(from_timedate_gmt, 1, length(from_timedate_gmt)-3)\"))\n",
    "    df_spark = df_spark.withColumn(\"to_timedate_gmt\", expr(\"substring(to_timedate_gmt, 1, length(to_timedate_gmt)-3)\"))\n",
    "    #df_spark = df_spark.withColumn(\"from_timedate_gmt\", df_spark.from_timedate_gmt.substr(1, 6))\n",
    "\n",
    "    df = df_spark.to_koalas()\n",
    "\n",
    "    #df_cache.spark.unpersist()\n",
    "\n",
    "    return df\n",
    "\n",
    "def square(s):\n",
    "    return s/3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c786d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, to_timestamp, col, unix_timestamp, first, sum, count\n",
    "\n",
    "# Add 'totimedate_plus_stoptime' column calculated by making the sum of to_date and the stoptime\n",
    "def add_new_stoptime_column(df):\n",
    "    #df_cache = df.spark.cache()\n",
    "    df_spark = df.to_spark()\n",
    "    \n",
    "    df_spark = df_spark.withColumn('new_stoptime', col('stoptime_s') + 60 - (col('stoptime_s') % 60))\\\n",
    "                       .withColumn('totimedate_plus_stoptime', (unix_timestamp(col('to_timedate_gmt').cast(TimestampType())) + col('new_stoptime')))\\\n",
    "                       .withColumn('totimedate_plus_stoptime', to_timestamp(col('totimedate_plus_stoptime')))\n",
    "\n",
    "    df_spark = df_spark.drop('new_stoptime')\n",
    "\n",
    "    df = df_spark.to_koalas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495bc3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 12:03:39 WARN Utils: Your hostname, gpu2-Standard-PC-Q35-ICH9-2009 resolves to a loopback address: 127.0.1.1; using 10.0.0.205 instead (on interface enp6s18)\n",
      "22/06/15 12:03:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gpu2/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/15 12:03:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/15 12:03:40 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------OUTPUT------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[10]\")\\\n",
    "        .appName(\"Pyspark\")\\\n",
    "        .config('spark.driver.memory', \"126g\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .config('spark.executor.cores', \"100\")\\\n",
    "        .config(\"spark.local.dir\", \"/home/gpu2/spark-temp\")\\\n",
    "        .getOrCreate()\n",
    "spark.catalog.clearCache()\n",
    "    \n",
    "    \n",
    "# Upload CSV datas\n",
    "df = ks.read_csv(pathCSV, sep=';', dtype = {'from_timedate':str, 'to_timedate':str})\n",
    "#df = df.spark.cache()\n",
    "#df = df.spark.repartition(80)\n",
    "df = preprocessing(df)\n",
    "#   print(df.head())\n",
    "#   exit()\n",
    "print('-----------OUTPUT------------')\n",
    "#df_cache = df.spark.cache()\n",
    "df = df.rename(columns={'from_zone':'from_zone_fid', 'from_timedate':'from_timedate_gmt', 'to_zone':'to_zone_fid', 'to_timedate':'to_timedate_gmt'})\n",
    "df = df.dropna(subset=['to_zone_fid'])\n",
    "fill = {'from_zone_fid': -1}\n",
    "df = df.fillna(value=fill)\n",
    "# Iitial Filter\n",
    "# (f.triptime_s < 3278)\n",
    "df = df.loc[ (df.tripdistance_m > 0) & (df.stoptime_s > 0)] # & (df.stoptime_s != 31422298)]\n",
    "df = df.sort_values(by=['idterm', 'to_timedate_gmt'])\n",
    "casa = df[df.triptime_s>8500].head(2)\n",
    "#df.stoptime_s = df.stoptime_s.apply(square)\n",
    "#test = df.stoptime_s.to_numpy()\n",
    "\n",
    "#ax = \n",
    "#df.stoptime_s.plot.kde(bw_method=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe11fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower limit of times in the dataset\n",
    "def time_limits_inf_2(df, intv):\n",
    "    #df_cache = dataframe.spark.cache()\n",
    "\n",
    "    # Generation of lower and upper bound of the calendar\n",
    "    min = ks.to_datetime(df.from_timedate_gmt.min(), format='%Y/%m/%d %H:%M')  # Lower date in the df\n",
    "    min = min.replace(minute=00)  # Set minutes to 00\n",
    "    max = ks.to_datetime(df.to_timedate_gmt.max(), format='%Y/%m/%d %H:%M')  # Upper date in the df\n",
    "    max = max.replace(hour=23, minute=59)  # Upper date set to 23:59\n",
    "\n",
    "    # Create calendar\n",
    "    cal = calendar(min, max, intv)\n",
    "    cal = cal.to_frame(index=False, name='date')\n",
    "    cal = cal.to_spark()\n",
    "    \n",
    "    #print(cal.show())\n",
    "    df_spark = df.to_spark()\n",
    "    \n",
    "    df_spark.createOrReplaceTempView('df_spark')\n",
    "    cal.createOrReplaceTempView('cal')\n",
    "    res1 = df_spark.join(cal).where((to_date(df_spark.from_timedate_gmt) == to_date(cal.date)) & \\\n",
    "                               (df_spark.from_timedate_gmt.cast(TimestampType()) >= cal.date.cast(TimestampType()))).withColumnRenamed('date','date_1')#.crossJoin(cal).withColumnRenamed('date', 'date_2')#.crossJoin(cal).withColumnRenamed('date', 'date_3')\n",
    "\n",
    "    res1 = res1.drop('from_timedate_gmt')#, 'to_timedate_gmt')#, 'date_2')\n",
    "    res1 = res1.withColumnRenamed('date_1', 'from_timedate_gmt')\n",
    "    w = Window.partitionBy(\"idtrajectory\").orderBy(col(\"from_timedate_gmt\").desc()) #, col(\"to_timedate_gmt\").desc())\n",
    "    res1 = res1.withColumn(\"row\", row_number().over(w)).where(\"row == 1\").drop(\"row\")\n",
    "    res1.createOrReplaceTempView('res1')\n",
    "    \n",
    "    res2 = res1.join(cal).where((to_date(df_spark.to_timedate_gmt) == to_date(cal.date)) & \\\n",
    "                               (df_spark.to_timedate_gmt.cast(TimestampType()) >= cal.date.cast(TimestampType()))).withColumnRenamed('date','date_1')\n",
    "    \n",
    "    res2 = res2.drop('to_timedate_gmt')#, 'to_timedate_gmt')#, 'date_2')\n",
    "    res2 = res2.withColumnRenamed('date_1', 'to_timedate_gmt')\n",
    "    w = Window.partitionBy(\"idtrajectory\").orderBy(col(\"to_timedate_gmt\").desc())\n",
    "    res2 = res2.withColumn(\"row\", row_number().over(w)).where(\"row == 1\").drop(\"row\")\n",
    "    dataframe = res2.to_koalas()\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b2b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path ShapeFile\n",
    "pathShapeF = 'data/shapefile/shapefile_2323.shp'\n",
    "\n",
    "# Add 'weight' column calculated by making the quotient between the chosen\n",
    "# interval and the delta of the time taken for the travel\n",
    "def add_peso_column(dataframe, calendario):\n",
    "    delta_cal = calendario.to_list()[1] - calendario.to_list()[0]\n",
    "    delta_cal = delta_cal.seconds\n",
    "\n",
    "    df_spark = dataframe.to_spark()\n",
    "    \n",
    "    df_spark = df_spark.withColumn('peso', col('to_timedate_gmt').cast(LongType()) - col('from_timedate_gmt').cast(LongType()))\\\n",
    "                       .withColumn('peso', delta_cal / (col('peso') + delta_cal))\n",
    "    \n",
    "\n",
    "    dataframe = df_spark.to_koalas()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1328bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator of calendar with specific timestamp (start/end='dd-mm-yyyy', freq='20min', '8hour', ...)\n",
    "def calendar(start, end, freq):\n",
    "    calendar = ks.date_range(start=start, end=end, freq=freq)\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ed6da4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/namespace.py:1754: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dopo limite "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 12:03:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:03:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idtrajectory  idterm  from_zone_fid  to_zone_fid  tripdistance_m  triptime_s  stoptime_s   from_timedate_gmt     to_timedate_gmt\n",
      "0     104214517    1432             -1         1006           29220       10262        5008 2013-04-17 11:00:00 2013-04-17 11:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/namespace.py:1754: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  pd.date_range(\n",
      "22/06/15 12:04:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idtrajectory  idterm  from_zone_fid  to_zone_fid  tripdistance_m  triptime_s  stoptime_s   from_timedate_gmt     to_timedate_gmt  peso\n",
      "0     104214517    1432             -1         1006           29220       10262        5008 2013-04-17 11:00:00 2013-04-17 11:00:00   1.0\n",
      "---ADJ--- 0\n",
      "2013-04-17 10:00:00\n",
      "2013-04-17 18:00:00\n",
      "---SHAPEADJ--- 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------+-----------+--------------+----------+----------+-------------------+-------------------+----+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|  from_timedate_gmt|    to_timedate_gmt|peso|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-------------------+-------------------+----+\n",
      "|   104214517|  1432|           -1|       1006|         29220|     10262|      5008|2013-04-17 11:00:00|2013-04-17 11:00:00| 1.0|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-------------------+-------------------+----+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ADJ--- 1\n",
      "2013-04-17 18:00:00\n",
      "2013-04-18 02:00:00\n",
      "---SHAPEADJ--- 1\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:04:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ADJ--- 2\n",
      "2013-04-18 02:00:00\n",
      "2013-04-18 10:00:00\n",
      "---SHAPEADJ--- 2\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.autocall.ZMQExitAutocall at 0x7f82dc7c9940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADJ matrix\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import concat_ws, row_number, date_format,from_unixtime,lag, col, lead, to_date, unix_timestamp, datediff, lit, split\n",
    "\n",
    "shapefile = gpd.read_file(pathShapeF)\n",
    "lista_zone = shapefile.FID.to_list()\n",
    "\n",
    "intv_1='8H'\n",
    "#intv_1 = '1H' if intv in ['2H', '3H', '4H', '5H', '6H', '7H', '8H', '9H','10H', '11H', '12H'] else intv\n",
    "# Lower bound of time interval\n",
    "casa = time_limits_inf_2(casa, intv_1)\n",
    "print('dopo limite',casa.head(1))\n",
    "\n",
    "index_zone = list(range(len(lista_zone)))           # List from 0 to -1 (zones)\n",
    "map_zone = dict(list(zip(lista_zone, index_zone)))  # Map each element with its position number\n",
    "    # Take-over calendar bounds\n",
    "min = df.from_timedate_gmt.min()\n",
    "max = df.to_timedate_gmt.max()\n",
    "cal = ks.date_range(start='2013-04-17 10:00', end='2013-04-18 16:00', freq=intv_1)\n",
    "#casa.to_timedate_gmt = ks.to_datetime(casa.to_timedate_gmt)\n",
    "#casa.from_timedate_gmt = ks.to_datetime(casa.from_timedate_gmt)\n",
    "\n",
    "casa = add_peso_column(casa, cal)\n",
    "print(casa.head(1))\n",
    "cal = cal.to_list()\n",
    "list_matrix_tot = []\n",
    "df_check = casa.to_spark().cache()\n",
    "lista_zone_1 = spark.createDataFrame(lista_zone, IntegerType()).select(col('value').alias('from_zone_fid'))\n",
    "lista_zone_2 = spark.createDataFrame(lista_zone, IntegerType()).select(col('value').alias('to_zone_fid'))\n",
    "crossListe = lista_zone_1.crossJoin(lista_zone_2)\n",
    "for j in range(0, len(cal) - 1):\n",
    "    print('---ADJ---', j)\n",
    "    print(cal[j])\n",
    "    print(cal[j+1])\n",
    "    dataframe_ridotto = df_check.filter(((df_check.from_timedate_gmt >= cal[j]) &\n",
    "                                  (df_check.from_timedate_gmt < cal[j + 1]))|\n",
    "                                    ((df_check.to_timedate_gmt >= cal[j]) &\n",
    "                                           (df_check.to_timedate_gmt <= cal[j + 1])) |\n",
    "                                    ((df_check.from_timedate_gmt < cal[j]) &\n",
    "                                           (df_check.to_timedate_gmt > cal[j + 1])))\n",
    "\n",
    "    #print(dataframe_ridotto.head())\n",
    "    print('---SHAPEADJ---', j)\n",
    "    df_spark = dataframe_ridotto#.to_spark()\n",
    "    print(df_spark.show())\n",
    "    df_spark = df_spark.withColumn(\"peso\", df_spark[\"peso\"].cast(LongType()))\n",
    "    df_grouped = df_spark.groupBy('from_zone_fid', 'to_zone_fid').agg(sum('peso').alias('peso'))\n",
    "    df_grouped = crossListe.join(df_grouped, on = ['from_zone_fid', 'to_zone_fid'], how='left').fillna(0)\n",
    "    matrix = df_grouped.select('peso').to_numpy().values.reshape((len(lista_zone), len(lista_zone)))\n",
    "    print(matrix)\n",
    "    matrix = (matrix.T / matrix.sum(axis=1)).T\n",
    "    matrix = np.nan_to_num(matrix)\n",
    "    matrix = csr_matrix(matrix)\n",
    "    list_matrix_tot += [matrix]\n",
    "    del matrix\n",
    "exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7015ed03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/namespace.py:1754: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  pd.date_range(\n",
      "22/06/15 12:05:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idtrajectory  idterm  from_zone_fid  to_zone_fid  tripdistance_m  triptime_s  stoptime_s   from_timedate_gmt     to_timedate_gmt  peso totimedate_plus_stoptime\n",
      "0     104214517    1432             -1         1006           29220       10262        5008 2013-04-17 11:00:00 2013-04-17 11:00:00   1.0      2013-04-17 12:24:00\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "---FEAT--- 0\n",
      "calendar 2013-04-17 13:00:00\n",
      "calendar 2013-04-17 14:00:00\n",
      "nuovooo \n",
      "\n",
      "---SHAPEADJ--- 0\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|totimedate_plus_stoptime|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.8551868539998395\n",
      "---FEAT--- 1\n",
      "calendar 2013-04-17 14:00:00\n",
      "calendar 2013-04-17 15:00:00\n",
      "nuovooo \n",
      "\n",
      "---SHAPEADJ--- 1\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|totimedate_plus_stoptime|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.7265331069993408\n",
      "---FEAT--- 2\n",
      "calendar 2013-04-17 15:00:00\n",
      "calendar 2013-04-17 16:00:00\n",
      "nuovooo \n",
      "\n",
      "---SHAPEADJ--- 2\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|totimedate_plus_stoptime|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.7348299139994197\n",
      "---FEAT--- 3\n",
      "calendar 2013-04-17 16:00:00\n",
      "calendar 2013-04-17 17:00:00\n",
      "nuovooo \n",
      "\n",
      "---SHAPEADJ--- 3\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|totimedate_plus_stoptime|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.7075623889995768\n",
      "---FEAT--- 4\n",
      "calendar 2013-04-17 17:00:00\n",
      "calendar 2013-04-17 18:00:00\n",
      "nuovooo \n",
      "\n",
      "---SHAPEADJ--- 4\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|totimedate_plus_stoptime|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.6949614910008677\n",
      "---FEAT--- 5\n",
      "calendar 2013-04-17 18:00:00\n",
      "calendar 2013-04-17 19:00:00\n",
      "nuovooo \n",
      "\n",
      "---SHAPEADJ--- 5\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "|idtrajectory|idterm|from_zone_fid|to_zone_fid|tripdistance_m|triptime_s|stoptime_s|from_timedate_gmt|to_timedate_gmt|peso|totimedate_plus_stoptime|\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "+------------+------+-------------+-----------+--------------+----------+----------+-----------------+---------------+----+------------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2/.local/lib/python3.8/site-packages/databricks/koalas/generic.py:603: UserWarning: We recommend using `DataFrame.to_numpy()` instead.\n",
      "  warnings.warn(\"We recommend using `{}.to_numpy()` instead.\".format(type(self).__name__))\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/15 12:05:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.698523869001292\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     list_matrix_tot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [matrix]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m matrix\n\u001b[0;32m---> 77\u001b[0m \u001b[43mexat\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exat' is not defined"
     ]
    }
   ],
   "source": [
    "# Transiction\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import concat_ws, row_number, date_format,from_unixtime,lag, col, lead, to_date, unix_timestamp, datediff, lit, split\n",
    "import timeit\n",
    "\n",
    "\n",
    "intv='1H'\n",
    "\n",
    "start = ks.to_datetime(casa.from_timedate_gmt.min(), format='%Y/%m/%d %H:%M')  # Lower date in the df\n",
    "start = start.replace(minute=00)  # Set minutes to 00\n",
    "end = ks.to_datetime(casa.to_timedate_gmt.max(), format='%Y/%m/%d %H:%M')  # Upper date in the df\n",
    "end = end.replace(hour=23, minute=59)  # Upper date set to 23:59\n",
    "calendar = ks.date_range(start='2013-04-17 13:00', end='2013-04-17 19:00', freq=intv)\n",
    "cal = calendar.to_list()\n",
    "cal = ks.to_datetime(cal)\n",
    "casa = add_new_stoptime_column(casa)\n",
    "print(casa.head(1))\n",
    "shapefile = gpd.read_file(pathShapeF)\n",
    "lista_zone = shapefile.FID.to_list()\n",
    "index_zone = list(range(len(lista_zone)))  # List from 0 to num zones\n",
    "map_zone = dict(list(zip(lista_zone, index_zone)))  # Map each element with its position number\n",
    "list_matrix_tot = []\n",
    "df_check = casa.to_spark().cache()\n",
    "lista_zone_2 = spark.createDataFrame(lista_zone, IntegerType()).select(col('value').alias('to_zone_fid'))\n",
    "print(type(df_check))\n",
    "for j in range(0, len(cal) - 1):\n",
    "    print('---FEAT---', j)\n",
    "    print('calendar', cal[j])\n",
    "    print('calendar', cal[j+1])\n",
    "          \n",
    "#   \n",
    "          # Dataset reduction with each time interval\n",
    "    #dataframe_ridotto = df_check.loc[((df_check.to_timedate_gmt >= cal[j]) &\n",
    "    #                                  (df_check.to_timedate_gmt < cal[j + 1])) |\n",
    "    #                                 ((df_check.totimedate_plus_stoptime > cal[j]) &\n",
    "    #                                  (df_check.totimedate_plus_stoptime <= cal[j + 1])) |\n",
    "    #                                 ((df_check.to_timedate_gmt < cal[j]) &\n",
    "    #                                  (df_check.totimedate_plus_stoptime > cal[j + 1]))]\n",
    "    dataframe_ridotto = df_check.filter(((df_check.to_timedate_gmt >= cal[j]) &\n",
    "                                      (df_check.to_timedate_gmt < cal[j + 1])) |\n",
    "                                     ((df_check.totimedate_plus_stoptime > cal[j]) &\n",
    "                                      (df_check.totimedate_plus_stoptime <= cal[j + 1])) |\n",
    "                                     ((df_check.to_timedate_gmt < cal[j]) &\n",
    "                                      (df_check.totimedate_plus_stoptime > cal[j + 1])))\n",
    "    #dataframe_ridotto = df_check.loc[((df_check.to_timedate_gmt >= cal[j]) &\n",
    "#   \n",
    "    #                                   (df_check.totimedate_plus_stoptime <= cal[j + 1]))]\n",
    "#   \n",
    "    #estraggo la prima riga per ogni id_term e id_pickup\n",
    "    print('nuovooo \\n')\n",
    "    #print(dataframe_ridotto.head(4))\n",
    "    #ataframe_ridotto.head(1) !=0:\n",
    "    start_time = timeit.default_timer()\n",
    "    #if len(dataframe_ridotto.head(1)) != 0:\n",
    "    print('---SHAPEADJ---', j)\n",
    "    df_spark = dataframe_ridotto #.to_spark()\n",
    "    print(df_spark.show())\n",
    "    df_grouped = df_spark.groupBy('to_zone_fid').agg(count('to_zone_fid').alias('count'))\n",
    "    df_grouped = lista_zone_2.join(df_grouped, on = ['to_zone_fid'], how='left').fillna(0)\n",
    "    matrix = df_grouped.select('count').to_koalas().values.reshape((len(lista_zone), 1))\n",
    "\n",
    "    #matrix = df_grouped.select('count')\n",
    "    #print(matrix.show())\n",
    "    #print((matrix.count(), len(matrix.columns)))\n",
    "    #matrix = np.concatenate(df_grouped.select('count').rdd.glom().map(lambda x: np.array([elem[0] for elem in x])).collect()).reshape((len(lista_zone), 1))\n",
    "    #matrix = df_grouped.select('count').write.save(path='csv', format='csv', mode='append', sep='\\t')\n",
    "    #to_koalas().values.reshape((len(lista_zone), 1))\n",
    "    #print(type(matrix))\n",
    "    #Your statements here\n",
    "    stop_time = timeit.default_timer()\n",
    "    print('Time: ', stop_time - start_time)\n",
    "    #print(matrix)\n",
    "    matrix = csr_matrix(matrix)\n",
    "    list_matrix_tot += [matrix]\n",
    "    del matrix\n",
    "exat\n",
    "    #else:\n",
    "    #    matrix = np.zeros((len(lista_zone), 1))\n",
    "    #    matrix = csr_matrix(matrix)\n",
    "    #    list_matrix_tot += [matrix]\n",
    "    #    del matrix\n",
    "    #    continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
